---
title: "AI ATAC 3 Challenge: Efficiency & Effectiveness Afforded by Security
  Orchestration & Automated Response (SOAR) Capabilities"
challenge-title: "AI ATAC 3 Challenge: Efficiency & Effectiveness Afforded by
  Security Orchestration & Automated Response (SOAR) Capabilities"
layout: front-matter-data-markdownify-content
permalink: /challenge/AI-ATAC-3-challenge/
challenge-id: "1193"
status: open
sidenav: true
card-image: /assets/netlify-uploads/dod_seal.jpg
agency-logo: /assets/netlify-uploads/dod_seal.jpg
tagline: Artificial Intelligence SOAR Tools for Security Operations Center (SOC)
  Automation
agency: U.S. Navy
partner-agencies-federal: Oak Ridge National Laboratories (ORNL)
total-prize-offered-cash: $750,000
type-of-challenge:
  - Software and apps
  - Technology demonstration and hardware
  - Analytics, visualizations, algorithms
submission-start: 2020/12/10 08:00 AM
submission-end: 2020/02/12 5:00 PM
fiscal-year: FY21
legal-authority: Agency specific prize authority
challenge-manager: John Armantrout
challenge-manager-email: John.Armantrout@navy.mil
point-of-contact: AIATAC.PRIZE.CHALLENGE@NAVY.MIL
description: >-
  #### General Overview


  The Naval Information Warfare Systems Command (NAVWARSYSCOM) and the Program Executive Office for Command, Control, Communications, Computers, and Intelligence (PEO C4I) are conducting a third instance of the Artificial Intelligence Applications to Autonomous Cybersecurity (AI ATAC, pronounced “AI attack”) Challenge (hereinafter referred to as “the Challenge”). The Navy’s Information Assurance and Cybersecurity Program Office (PMW 130) seeks to enhance the Security Operations Center (SOC) using automated artificial intelligence and machine learning (AI/ML) tools to automate the detection and prevention of advanced persistent threat (APT) and other cybersecurity campaign activity. Current SOC operations require a tremendous amount of time and effort to triage alerts, link related logs, perform incident response, and document investigations. Through this AI ATAC Prize Challenge, PMW 130 is soliciting Security Orchestration & Automated Response (SOAR) tools that use AI/ML to enhance their effectiveness for competitive evaluation. Submissions must be accompanied by a submission description white-paper plus an overview video and a demonstration video describing the SOAR tool and associated technologies.


  This Challenge seeks to evaluate the utility of SOAR tools for NAVWAR security operation center (SOC) teams. SOAR tools as used in this challenge are technologies to coordinate, manage, and automate an organization’s SOC and federate an organization’s security processes, workflows, and procedures to provide a centralized, coordinated security posture for that organization. SOAR tools should significantly enhance the ability of security operators and analysts to perform their tasks (e.g., alert handling, ticket processing, threat detection, incident response, and post-compromise forensics) by providing machine-powered assistance to human analysts to improve the efficiency and consistency of people and processes. For instance, workflows or playbooks define the set of steps to address a potential threat, and SOAR tools may execute these steps automatically or guide a human analyst through them. SOAR tools using AI/ML should greatly enhance these benefits.


  For the purposes of this competition, SOAR tools are defined as tools that perform, coordinate, and automate the following actions: 


  * Ingest logs and alerts from a wide variety of security tools in a SOC;

  * Ingest or provide threat intelligence information from both internal and external sources;

  * Combine, coordinate, and enrich logging, alert, and threat data in the tool’s User Interface (UI);

  * Identify common attack patterns and highlight them in the tool’s UI;

  * Automate and ultimately simplify the alert triage and incident response processes via preset and configurable workflows or playbooks;

  * Facilitate simultaneous or asynchronous (e.g., via sharing or handing off) investigations by different, potentially geographically disparate operators, with access to potentially different types of information; and

  * Automate and expedite documentation of triage, incident response, and forensics (e.g., via ticketing). 


  This Challenge will measure the performance of these tools to determine how well they would provide improvements to U.S. Navy SOCs across the world.


  #### Key Dates


  * 10 December 2020: Kick-off of AI ATAC 3 on Challenge.gov

  * 8 January 2021: Deadline for potential Participants to send an email to indicate intent to participate (details below)

  * 12 February 2021:	All submissions due

  * TBD: Candidates selected for Phase 2 will be notified by email
prizes: >-
  #### Total Cash Prize Purse


  $750,000


  #### Prize Breakdown


  NAVWARSYSCOM has established $750,000 as the total amount set aside for cash prizes under the Challenge. In the case of a single winner, the entire cash prize will be awarded to the winning entry. In the unlikely event of a tie, NAVWARSYSCOM will determine an equitable method of distributing the cash prizes. If a prize goes to a team of Participants, NAVWARSYSCOM will award the cash prize to the individual/team’s point of contact registered on the Challenge.gov website. 


  Tax treatment of prizes will be handled in accordance with U.S. Internal Revenue Service guidelines. The winner must provide a U.S. Taxpayer Identification Number (e.g., a SSN, TIN, EIN) to receive the cash prize.


  #### Non-monetary Prizes


  NAWARSYSCOM may award, pursuant to Title 10 U.S.C. § 2371b, a follow-on production contract or transaction to one or more Participants who successfully demonstrated an effective AI/ML SOAR capability under the Challenge. The Challenge, however, does not in any way obligate NAVWARSYSCOM to procure any of the items within the terms or bounds of this Challenge from any Participant, including from the Challenge winner(s).
rules: >-
  Each Participant (individual Participant, team of Participants, or commercial
  entity) shall submit one entry in response to this Challenge. Team or
  commercial entity entries must identify one individual as a primary point of
  contact (POC) and prize recipient. By submitting an entry, a Participant
  authorizes his or her name to be released to the media if the Participant wins
  the prize.


  While reading this section, it may be helpful to refer to the “How to Enter” (Section 5), which itemizes the detailed requirements of a submission package. Section 5b provides instructions for submitting the package.

  This Challenge will be conducted in two successive phases. 


  **Phase 1** 

  The Phase 1 submission package will provide the following:


  1. Submission description White-paper (document);

  2. Overview and Demonstration videos (each less than 10 minutes long);

  3. The SOAR technology itself (via VM, hardware, and/or any included and necessary software); and,

  4. A setup guide (document). See Submission Requirements below for the specifics of each submission component.


  Phase 1 will be used to identify the most promising candidate technologies that will be selected for and advanced to Phase 2 evaluation. These selections will be based on the technology’s Whitepaper document plus the Overview and Demonstration videos. SOAR tool submitters must be available during Phase 1 for any questions by the U.S. Navy and Oak Ridge National Laboratory (ORNL) teams.


  PMW 130 will notify all Contestants that advance to Phase 2 using the contact information provided in the submission. The Government, may, but is not necessarily required to, notify Contestants that do not advance.  


  **Phase 2**

  Phase 2 will involve more in-depth evaluation of candidate technologies by subject matter experts at ORNL and actual SOC operators from both ORNL and the U.S. Navy. It will require a fifth and final submission component, namely (5) a User Tutorial video. Upon notification of selection for Phase 2, further instructions and a deadline will be provided to only the selected technologies for the (5) User Tutorial video. 


  In addition, technical assistance to ORNL’s team is expected for ensuring expedient and proper set-up of the submitted SOAR tool into ORNL’s environment during Phase 2. Remote support (video- or tele-conferences) will be scheduled with the submission team’s technical point of contact (POC) specified in the Phase 1 submission package.


  Once technologies are appropriately configured, Phase 2 evaluation will commence. Phase 2 will include a hands-on test of the SOAR tool, conducted by multiple SOC operators, while red-team attack campaigns are administered. The environment will include relevant background traffic and real users populating the test network. Tests involving collaboration of multiple operators using the SOAR tool may be conducted, both for operators working in the same network SOC and when handing off investigations to operators in a different network’s SOC.


  Expertise of SOC analysts is particularly important and relevant to Phase 2, and access to SOC expertise is limited; consequently, Phase 2 will be evaluated on a smaller subset of Phase 1 Participants. Additional tests of each SOAR tool, to measure further quantitative and qualitative metrics, will be performed by ORNL research staff.


  All questions regarding the Challenge should be sent via email to [AIATAC.PRIZE.CHALLENGE@NAVY.MIL](mailto:AIATAC.PRIZE.CHALLENGE@NAVY.MIL) no later than 9 January 2021, 1700 EST. Questions submitted after this deadline may not be addressed.


  #### Eligibility Requirements 


  The Challenge is open to individual Participants, teams of Participants, and commercial entities. As part of its submission, Participants must either own the intellectual property (IP) in the solution or provide documentation that indicates all IP stakeholders in its submission. The documentation should describe the type of IP and the entity that holds title to the IP. In either case, only one entry for each commercial technology is allowed. Commercial entities must be incorporated in and maintain a primary place of business in the United States (U.S.). Individual Participants and all members of teams of Participants must all be U.S. citizens or U.S. Permanent Residents and be 18 years or older as of 15 December 2020. All Participants (commercial entities or individuals) must have a Social Security Number (SSN), Taxpayer Identification Number (TIN), or Employer Identification Number (EIN) in order to receive a prize. Eligibility is subject to verification before any prize is awarded.

  Federal Government employees, PMW 130 support contractors and their employees, and Oak Ridge National Laboratory (ORNL) support contractors and their employees are not eligible to participate in this Challenge. Violation of the rules contained herein or intentional or consistent activity that undermines the spirit of the Challenge may result in disqualification. The Challenge is void wherever restricted or prohibited by law.


  #### Terms & Conditions 


  These terms and conditions apply to all Participants in the Challenge.


  The Participant agrees to comply with and be bound by the AI ATAC Challenge Background and Rules (“the Rules”) as well as the Terms and Conditions contained herein. The Participant also agrees that the decisions of the Government, in connection with all matters relating to this Challenge, are binding and final.


  The Participant agrees to follow and comply with all applicable federal, state and local laws, regulations, and policies.


  This Challenge is subject to all applicable federal laws and regulations. ALL CLAIMS ARISING OUT OF OR RELATING TO THESE TERMS WILL BE GOVERNED BY THE FEDERAL LAWS AND REGULATIONS OF THE UNITED STATES OF AMERICA.


  #### Data Rights


  NAVWARSYSCOM does not require that Participants relinquish or otherwise grant license rights to intellectual property developed or delivered under the Challenge. NAVWARSYSCOM requires sufficient data rights and intellectual property rights to use, release, display, and disclose the white paper and tool, but only to the evaluation team members, and only for purposes of evaluating the Participant submission. The evaluation team does not plan to retain entries after the Challenge is completed, but does plan to retain data and aggregate performance statistics resulting from the evaluation of those entries. By accepting these Terms and Conditions, the Participant consents to the use of data submitted to the evaluation team for these purposes.


  NAVWARSYSCOM may contact Participants, at no additional cost to the Government, to discuss the means and methods used in solving the Challenge, even if Participants did not win the Challenge. Such contact does not imply any sort of contractual commitment with the Participant.


  Because of the number of anticipated Challenge entries, NAVWARSYSCOM cannot and will not make determinations on whether or not third-party materials in the Challenge submissions have protectable intellectual property interests. By participating in this Challenge, each Participant (whether participating individually, as a team, or as a commercial entity) warrants and assures the Government that any data used for the purpose of submitting an entry for this Challenge were obtained legally and through authorized access to such data. By entering the Challenge and submitting the Challenge materials, the Participant agrees to indemnify and hold the Government harmless against any claim, loss or risk of loss for patent or copyright infringement with respect to such third-party interests.


  This Challenge does not replace or supersede any other written contracts and/or written challenges that the Participant has or will have with the Government, which may require delivery of any materials the Participant is submitting herein for this Challenge effort. 


  This Challenge constitutes the entire understanding of the parties with respect to the Challenge. NAVWARSYSCOM may update the terms of the Challenge from as needed without notice. Participants are strongly encouraged to check the Challenge.gov website frequently. 


  If any provision of this Challenge is held to be invalid or unenforceable under applicable federal law, it will not affect the validity or enforceability of the remainder of the Terms and Conditions of this Challenge.


  #### Results of Challenge


  The Challenge winners will be announced on the Challenge.gov website and via email. NAVWARSYSCOM will announce the winners via appropriate channels. If winners receive notification prior to public announcement, winners shall agree not to disclose its winning status until after the Government releases its announcement.


  #### Release of Claims


  The Participant agrees to release and forever discharge any and all manner of claims, equitable adjustments, actions, suits, debts, appeals, and all other obligations of any kind, whether past or present, known or unknown, that have or may arise from, are related to or are in connection with, directly or indirectly, this Challenge or the Participant’s submission.
judging: >-
  #### Judging Panel


  Both Phase 1 and Phase 2 of this Challenge will be reviewed by members of the ORNL Cybersecurity Research Group and/or U.S. Navy subject matter experts.


  #### Judging Criteria


  Submissions will be judged based on how they address the SOAR capability criteria below: 


  1. Ability to ingest custom logging / alerts

  2. Playbook / Workflow Use: 

     * Usefulness of existing playbooks
     * Ability of junior operators to effectively use workflows/playbooks
     * Efficiency gains by using workflows/playbooks
  3. Playbook / Workflow customization: 

     * Easy and flexible creation of custom workflows/playbooks
     * Shareability of workflows/playbooks
  4. Task Automation: 

     * Quickly resolving and documenting false positive alerts
     * Ability to automate common tasks such as responding to phishing attacks and failed user logins
     * Effectiveness of automation
     * Efficiency gains from automation 
  5. Documentation Automation: 

     * Ability to prepopulate alert and logging data into tickets
     * Ability to collect all needed data for elevating events to incidents and handing incident to higher tier / higher experience SOC personnel 
  6. Ability to rank / score alerts so that analysts can easily prioritize alerts from most to least significant

  7. Collaboration facilitation: 

     * Shareability of in-progress and completed investigations 
     * Real time collaboration 
     * Asynchronous collaboration (hand offs) 
     * Ability to provide metrics on how much time / money the SOAR tool is saving the organization
  8. Unique features: Unique features of the SOAR tool may be considered on the basis of facilitating functionality or adding value to the SOC.


  To select which tools will advance from Phase 1 to Phase 2, ORNL will conduct a comparative analysis of the submitted white-papers to determine how well they meet the above specified criteria. Navy SOC analysts will review the provided demonstration videos and provide feedback to ORNL on which tools they are most interested in seeing tested based on the SOAR capability criteria listed above.


  In Phase 2, ORNL will conduct an analysis of each tool’s ability to rank alerts, ingest data (both non-standard formats and widely supported formats), facilitate playbook creation and execution, automate ticket population and common tasks, and facilitate communication between potentially geographically separated SOCs. The winner(s) will be the submission whose cumulative score across all of these areas is highest.
---
### Description

#### General Overview

The Naval Information Warfare Systems Command (NAVWARSYSCOM) and the Program Executive Office for Command, Control, Communications, Computers, and Intelligence (PEO C4I) are conducting a third instance of the Artificial Intelligence Applications to Autonomous Cybersecurity (AI ATAC, pronounced “AI attack”) Challenge (hereinafter referred to as “the Challenge”). The Navy’s Information Assurance and Cybersecurity Program Office (PMW 130) seeks to enhance the Security Operations Center (SOC) using automated artificial intelligence and machine learning (AI/ML) tools to automate the detection and prevention of advanced persistent threat (APT) and other cybersecurity campaign activity. Current SOC operations require a tremendous amount of time and effort to triage alerts, link related logs, perform incident response, and document investigations. Through this AI ATAC Prize Challenge, PMW 130 is soliciting Security Orchestration & Automated Response (SOAR) tools that use AI/ML to enhance their effectiveness for competitive evaluation. Submissions must be accompanied by a submission description white-paper plus an overview video and a demonstration video describing the SOAR tool and associated technologies.

This Challenge seeks to evaluate the utility of SOAR tools for NAVWAR security operation center (SOC) teams. SOAR tools as used in this challenge are technologies to coordinate, manage, and automate an organization’s SOC and federate an organization’s security processes, workflows, and procedures to provide a centralized, coordinated security posture for that organization. SOAR tools should significantly enhance the ability of security operators and analysts to perform their tasks (e.g., alert handling, ticket processing, threat detection, incident response, and post-compromise forensics) by providing machine-powered assistance to human analysts to improve the efficiency and consistency of people and processes. For instance, workflows or playbooks define the set of steps to address a potential threat, and SOAR tools may execute these steps automatically or guide a human analyst through them. SOAR tools using AI/ML should greatly enhance these benefits.

For the purposes of this competition, SOAR tools are defined as tools that perform, coordinate, and automate the following actions: 

* Ingest logs and alerts from a wide variety of security tools in a SOC;
* Ingest or provide threat intelligence information from both internal and external sources;
* Combine, coordinate, and enrich logging, alert, and threat data in the tool’s User Interface (UI);
* Identify common attack patterns and highlight them in the tool’s UI;
* Automate and ultimately simplify the alert triage and incident response processes via preset and configurable workflows or playbooks;
* Facilitate simultaneous or asynchronous (e.g., via sharing or handing off) investigations by different, potentially geographically disparate operators, with access to potentially different types of information; and
* Automate and expedite documentation of triage, incident response, and forensics (e.g., via ticketing). 

This Challenge will measure the performance of these tools to determine how well they would provide improvements to U.S. Navy SOCs across the world.

#### Key Dates

* 10 December 2020: Kick-off of AI ATAC 3 on Challenge.gov
* 8 January 2021: Deadline for potential Participants to send an email to indicate intent to participate (details below)
* 12 February 2021:	All submissions due
* TBD: Candidates selected for Phase 2 will be notified by email

### Prizes

#### Total Cash Prize Purse

$750,000

#### Prize Breakdown

NAVWARSYSCOM has established $750,000 as the total amount set aside for cash prizes under the Challenge. In the case of a single winner, the entire cash prize will be awarded to the winning entry. In the unlikely event of a tie, NAVWARSYSCOM will determine an equitable method of distributing the cash prizes. If a prize goes to a team of Participants, NAVWARSYSCOM will award the cash prize to the individual/team’s point of contact registered on the Challenge.gov website. 

Tax treatment of prizes will be handled in accordance with U.S. Internal Revenue Service guidelines. The winner must provide a U.S. Taxpayer Identification Number (e.g., a SSN, TIN, EIN) to receive the cash prize.

#### Non-monetary Prizes

NAWARSYSCOM may award, pursuant to Title 10 U.S.C. § 2371b, a follow-on production contract or transaction to one or more Participants who successfully demonstrated an effective AI/ML SOAR capability under the Challenge. The Challenge, however, does not in any way obligate NAVWARSYSCOM to procure any of the items within the terms or bounds of this Challenge from any Participant, including from the Challenge winner(s).

### Rules

Each Participant (individual Participant, team of Participants, or commercial entity) shall submit one entry in response to this Challenge. Team or commercial entity entries must identify one individual as a primary point of contact (POC) and prize recipient. By submitting an entry, a Participant authorizes his or her name to be released to the media if the Participant wins the prize.

While reading this section, it may be helpful to refer to the “How to Enter” (Section 5), which itemizes the detailed requirements of a submission package. Section 5b provides instructions for submitting the package.
This Challenge will be conducted in two successive phases. 

**Phase 1** 
The Phase 1 submission package will provide the following:

1. Submission description White-paper (document);
2. Overview and Demonstration videos (each less than 10 minutes long);
3. The SOAR technology itself (via VM, hardware, and/or any included and necessary software); and,
4. A setup guide (document). See Submission Requirements below for the specifics of each submission component.

Phase 1 will be used to identify the most promising candidate technologies that will be selected for and advanced to Phase 2 evaluation. These selections will be based on the technology’s Whitepaper document plus the Overview and Demonstration videos. SOAR tool submitters must be available during Phase 1 for any questions by the U.S. Navy and Oak Ridge National Laboratory (ORNL) teams.

PMW 130 will notify all Contestants that advance to Phase 2 using the contact information provided in the submission. The Government, may, but is not necessarily required to, notify Contestants that do not advance.  

**Phase 2**
Phase 2 will involve more in-depth evaluation of candidate technologies by subject matter experts at ORNL and actual SOC operators from both ORNL and the U.S. Navy. It will require a fifth and final submission component, namely (5) a User Tutorial video. Upon notification of selection for Phase 2, further instructions and a deadline will be provided to only the selected technologies for the (5) User Tutorial video. 

In addition, technical assistance to ORNL’s team is expected for ensuring expedient and proper set-up of the submitted SOAR tool into ORNL’s environment during Phase 2. Remote support (video- or tele-conferences) will be scheduled with the submission team’s technical point of contact (POC) specified in the Phase 1 submission package.

Once technologies are appropriately configured, Phase 2 evaluation will commence. Phase 2 will include a hands-on test of the SOAR tool, conducted by multiple SOC operators, while red-team attack campaigns are administered. The environment will include relevant background traffic and real users populating the test network. Tests involving collaboration of multiple operators using the SOAR tool may be conducted, both for operators working in the same network SOC and when handing off investigations to operators in a different network’s SOC.

Expertise of SOC analysts is particularly important and relevant to Phase 2, and access to SOC expertise is limited; consequently, Phase 2 will be evaluated on a smaller subset of Phase 1 Participants. Additional tests of each SOAR tool, to measure further quantitative and qualitative metrics, will be performed by ORNL research staff.

All questions regarding the Challenge should be sent via email to [AIATAC.PRIZE.CHALLENGE@NAVY.MIL](mailto:AIATAC.PRIZE.CHALLENGE@NAVY.MIL) no later than 9 January 2021, 1700 EST. Questions submitted after this deadline may not be addressed.

#### Eligibility Requirements 

The Challenge is open to individual Participants, teams of Participants, and commercial entities. As part of its submission, Participants must either own the intellectual property (IP) in the solution or provide documentation that indicates all IP stakeholders in its submission. The documentation should describe the type of IP and the entity that holds title to the IP. In either case, only one entry for each commercial technology is allowed. Commercial entities must be incorporated in and maintain a primary place of business in the United States (U.S.). Individual Participants and all members of teams of Participants must all be U.S. citizens or U.S. Permanent Residents and be 18 years or older as of 15 December 2020. All Participants (commercial entities or individuals) must have a Social Security Number (SSN), Taxpayer Identification Number (TIN), or Employer Identification Number (EIN) in order to receive a prize. Eligibility is subject to verification before any prize is awarded.
Federal Government employees, PMW 130 support contractors and their employees, and Oak Ridge National Laboratory (ORNL) support contractors and their employees are not eligible to participate in this Challenge. Violation of the rules contained herein or intentional or consistent activity that undermines the spirit of the Challenge may result in disqualification. The Challenge is void wherever restricted or prohibited by law.

#### Terms & Conditions 

These terms and conditions apply to all Participants in the Challenge.

The Participant agrees to comply with and be bound by the AI ATAC Challenge Background and Rules (“the Rules”) as well as the Terms and Conditions contained herein. The Participant also agrees that the decisions of the Government, in connection with all matters relating to this Challenge, are binding and final.

The Participant agrees to follow and comply with all applicable federal, state and local laws, regulations, and policies.

This Challenge is subject to all applicable federal laws and regulations. ALL CLAIMS ARISING OUT OF OR RELATING TO THESE TERMS WILL BE GOVERNED BY THE FEDERAL LAWS AND REGULATIONS OF THE UNITED STATES OF AMERICA.

#### Data Rights

NAVWARSYSCOM does not require that Participants relinquish or otherwise grant license rights to intellectual property developed or delivered under the Challenge. NAVWARSYSCOM requires sufficient data rights and intellectual property rights to use, release, display, and disclose the white paper and tool, but only to the evaluation team members, and only for purposes of evaluating the Participant submission. The evaluation team does not plan to retain entries after the Challenge is completed, but does plan to retain data and aggregate performance statistics resulting from the evaluation of those entries. By accepting these Terms and Conditions, the Participant consents to the use of data submitted to the evaluation team for these purposes.

NAVWARSYSCOM may contact Participants, at no additional cost to the Government, to discuss the means and methods used in solving the Challenge, even if Participants did not win the Challenge. Such contact does not imply any sort of contractual commitment with the Participant.

Because of the number of anticipated Challenge entries, NAVWARSYSCOM cannot and will not make determinations on whether or not third-party materials in the Challenge submissions have protectable intellectual property interests. By participating in this Challenge, each Participant (whether participating individually, as a team, or as a commercial entity) warrants and assures the Government that any data used for the purpose of submitting an entry for this Challenge were obtained legally and through authorized access to such data. By entering the Challenge and submitting the Challenge materials, the Participant agrees to indemnify and hold the Government harmless against any claim, loss or risk of loss for patent or copyright infringement with respect to such third-party interests.

This Challenge does not replace or supersede any other written contracts and/or written challenges that the Participant has or will have with the Government, which may require delivery of any materials the Participant is submitting herein for this Challenge effort. 

This Challenge constitutes the entire understanding of the parties with respect to the Challenge. NAVWARSYSCOM may update the terms of the Challenge from as needed without notice. Participants are strongly encouraged to check the Challenge.gov website frequently. 

If any provision of this Challenge is held to be invalid or unenforceable under applicable federal law, it will not affect the validity or enforceability of the remainder of the Terms and Conditions of this Challenge.

#### Results of Challenge

The Challenge winners will be announced on the Challenge.gov website and via email. NAVWARSYSCOM will announce the winners via appropriate channels. If winners receive notification prior to public announcement, winners shall agree not to disclose its winning status until after the Government releases its announcement.

#### Release of Claims

The Participant agrees to release and forever discharge any and all manner of claims, equitable adjustments, actions, suits, debts, appeals, and all other obligations of any kind, whether past or present, known or unknown, that have or may arise from, are related to or are in connection with, directly or indirectly, this Challenge or the Participant’s submission.

### Judging Criteria

#### Judging Panel

Both Phase 1 and Phase 2 of this Challenge will be reviewed by members of the ORNL Cybersecurity Research Group and/or U.S. Navy subject matter experts.

#### Judging Criteria

Submissions will be judged based on how they address the SOAR capability criteria below: 

1. Ability to ingest custom logging / alerts
2. Playbook / Workflow Use: 

   * Usefulness of existing playbooks
   * Ability of junior operators to effectively use workflows/playbooks
   * Efficiency gains by using workflows/playbooks
3. Playbook / Workflow customization: 

   * Easy and flexible creation of custom workflows/playbooks
   * Shareability of workflows/playbooks
4. Task Automation: 

   * Quickly resolving and documenting false positive alerts
   * Ability to automate common tasks such as responding to phishing attacks and failed user logins
   * Effectiveness of automation
   * Efficiency gains from automation 
5. Documentation Automation: 

   * Ability to prepopulate alert and logging data into tickets
   * Ability to collect all needed data for elevating events to incidents and handing incident to higher tier / higher experience SOC personnel 
6. Ability to rank / score alerts so that analysts can easily prioritize alerts from most to least significant
7. Collaboration facilitation: 

   * Shareability of in-progress and completed investigations 
   * Real time collaboration 
   * Asynchronous collaboration (hand offs) 
   * Ability to provide metrics on how much time / money the SOAR tool is saving the organization
8. Unique features: Unique features of the SOAR tool may be considered on the basis of facilitating functionality or adding value to the SOC.

To select which tools will advance from Phase 1 to Phase 2, ORNL will conduct a comparative analysis of the submitted white-papers to determine how well they meet the above specified criteria. Navy SOC analysts will review the provided demonstration videos and provide feedback to ORNL on which tools they are most interested in seeing tested based on the SOAR capability criteria listed above.

In Phase 2, ORNL will conduct an analysis of each tool’s ability to rank alerts, ingest data (both non-standard formats and widely supported formats), facilitate playbook creation and execution, automate ticket population and common tasks, and facilitate communication between potentially geographically separated SOCs. The winner(s) will be the submission whose cumulative score across all of these areas is highest.

How to Enter