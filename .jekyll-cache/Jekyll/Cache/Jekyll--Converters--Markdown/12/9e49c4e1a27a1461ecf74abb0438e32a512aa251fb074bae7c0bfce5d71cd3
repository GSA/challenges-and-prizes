I"4<div class="judging-criterias"> <div> <h4>Judging Panels </h4> <p>Three judging panels will operate in a chronological series. Panels will be composed of a diverse group of individuals with specialized skills and experience relevant to the evaluation periods&rsquo; objectives. Each individual judge is appointed by the NIST Director, and required to sign a non-disclosure agreement (NDA) and conflict of interest (COI) disclosure to ensure that all participant materials are held in confidence and all judging is fair for participants.</p> <p>The Judge panel consists of experts who will evaluate the participants&rsquo; submissions according to the criteria defined for each contest. The objective of the judging will be to rank and select the top submissions in each period of evaluation. The decisions of the judges for the contest will be announced in accordance with the dates noted in the "Important Dates" section of these rules.</p> <p>In support of the judges and the evaluation process, reviewers are subject matter experts covered by NDA and COI requirements, who focus on implementing specific elements of the evaluation process. Reviewers provide evaluation support by assisting with the facilitation of the contests, implementing test and assessment activities, and other processes that generate data for the consideration of the judges, who are responsible for considering all the information and making the final decisions.</p> <p><strong>Panel 1:</strong> Regional Codeathon Judging Panel</p> <p>The Regional Codeathon Judging Panel will be established to support the evaluation of submissions at each Regional Codeathon event. The judges will coordinate between individual codeathon events, in burst 1 and burst 2, to ensure consistency and uniform implementation of evaluation process.</p> <p><strong>Panel 2:</strong> Online Contest Judging Panel</p> <p>The Online Contest Judging Panel will be established to support the consolidated evaluation of online submissions and the &ldquo;best and final&rdquo; updated versions of Regional Codeathon submissions. The consolidated evaluation will be completed between November 16, 2019 and January 27, 2020 through an online judging process.</p> <p><strong>Panel 3:</strong> National Award Event Contest Judging Panel</p> <p>The National Judging Panel will be established to select the National Winners of the Tech to Protect Challenge. The final evaluation will be completed in-person at the National Award Event in April 2020.</p> <p><strong>Panel 4:</strong> National Award Event Contest Judging Panel</p> <p>Following the completion of the Seed Contest, this Judging Panel will convene to evaluate the final submissions from Seed Contest Winners, and determine the Progress Contests Winners.</p> </div> <div> <h4>Evaluation Criteria </h4> <p>What determines the winners? This challenge includes ten unique contests and the evaluation process is designed to be as straightforward and simple as possible for participants to navigate. The graphic below represents the evaluation process that will be applied universally across all contests.</p> <p>Each contest includes evaluation criteria 0-5. Criteria 0 is a basic pass/fail compliance checklist of minimum requirements for submissions. Beyond the minimum requirements check, contests include up to five technical criteria used to evaluate submissions. Each technical criterion is weighted at 10-40 points out of a total of 100 points. Criteria items 0 through 3 will be used to evaluate Regional Codeathon and Online Contest submissions. Criteria items 4 and 5 represent the most advanced criteria items and will be used to evaluate the top submissions from the Online Contest, prior to selecting the National Finalists. Each of these five criteria items represent different features and attributes that the participant submissions will need to clearly demonstrate to the judges. The same evaluation process will be used throughout the challenge, with variation by contest for criteria 4-5. Each contest area has unique criteria, but in general examples may include aspects such as the application&rsquo;s UI/UX operability, accuracy, effectiveness, relevance to the contest&rsquo;s sample use case, and other similar elements.</p> <p><img src="{{ site.baseurl }}/assets/images/challenge-content/TTP_evaluation_table.jpg" width="671" height="243" alt="challenge evaluation table" /></p> </div> <div> <h4>Regional Codeathon Evaluation </h4> <p><strong>Objectives:</strong></p> <ul> <li>Selection of up to the top 4 submissions per Regional Codeathon event.</li> </ul> <ul> <li>Selection of one submission per contest (up to ten total) to be recognized at each Regional Codeathon event.</li> </ul> <p><strong>Process:</strong></p> <p>Regional Codeathon submissions will be evaluated by judges and reviewers at the end of each 48-hour codeathon event. <strong>Codeathon submissions will be evaluated against criteria items 0 through 3</strong> for a total of 60 rating points out of 100. A specific judging approach will be followed at the codeathon event, as described below. Participants will not be provided oral or written feedback based on the evaluation of their submissions.</p> <h4>Judging Phases</h4> <p><strong><span>Phase 1 - Initial compliance check</span></strong></p> <p><span>Criteria \#0</span></p> <p><strong><span>Objective:</span></strong><span> Initial assessment on evaluation criteria /#0 items</span></p> <p><strong>Evaluation Process</strong></p>
<p>● During the last day of each codeathon event, before the code freeze submission deadline, judges will review the progress of the teams towards meeting the criteria requirements of each respective contest. Judges will complete this initial review by interacting with participants. Judges will ask two questions to each participant. Judges will take notes on the progress of each participant at this stage during this initial assessment.</p> <hr /> <p><strong><span>Phase 2 - Rapid voting at code freeze </span></strong></p> <p><span>Criteria \#0-3</span></p> <p><strong><span>Objective:</span></strong><span> 3-minute review of each project&rsquo;s capabilities and selection of 4 projects or more to proceed to the next phase of evaluation.</span></p> <p><strong>Evaluation Process</strong></p> <p>● Quick, live demonstrations by the participating teams will be held in person at each participant&rsquo;s station during the rapid voting phase of each Regional Codeathon event. These demonstrations should happen right after the coding freeze deadline and last no longer than 3 minutes each. Participant teams should showcase the capabilities of the solutions and prototypes developed during this phase. SMEs and judges will rotate station to station to review the project demonstrations by the various teams. &lt;/span&gt;</p> <p>● SME reviewers and judges will evaluate participant submissions by inputting a simple yes or no for each project in a form provided by the national organizers. These votes should be an approximate reflection of whether the teams have met the criteria items 0 to 3, as described in each of the unique contests.&lt;/span&gt;</p> <p>● These ratings will be utilized to narrow down the list of teams that present projects during the next phase.&lt;/span&gt;</p> <hr /> <p><strong><span>Phase 3 - Judging Part A</span></strong></p> <p><strong><span>Participant presentations and judging</span></strong></p> <p><span>Criteria \#0-3</span></p> <p><strong><span>Objective:</span></strong><span> Teams present their projects in front of judges and are evaluated based on criteria items 0 to 3 per unique contest. </span></p> <p><strong>Evaluation Process</strong></p> <p>● Based on the initial voting, judges will invite teams to present their projects through a live presentation and PowerPoint slides. The presentations should take no more than 3 minutes, with an additional 3 minutes being available for questions and answers by the judges.&lt;/span&gt;</p> <p>● Judges will evaluate participant submissions based on criteria items 0 to 3. This evaluation is unique to each contest and may require interactions in addition to the presentation to confirm technical or other elements of the participant&rsquo;s submission.&lt;/span&gt;</p> <p>● Reviewers and judges will evaluate the participant&rsquo;s submission by inputting ratings in a form provided by the national organizers. &lt;/span&gt;</p> <hr /> <p><strong><span>Phase 4 - Judging Part B </span></strong></p> <p><strong><span>Desk review of the submissions and final judging</span></strong></p> <p><span>Criteria \#0-3</span></p> <p><strong><span>Objective:&nbsp; </span></strong><span>Detailed desk review of the submissions by the judges. This review will be done remotely, not at the codeathon event. </span></p> <p><strong>Evaluation Process</strong></p> <p>● Reviewers and judges will be assigned a set of submissions to evaluate. They will evaluate the submission materials and rank them based on criteria items 0 to 3 of the respective unique contest. &lt;/span&gt;</p> <p>● Reviewers and judges will have 2 days after the Regional Codeathon event to submit their rankings. &lt;/span&gt;</p> <p>● Regional Codeathon winners will be formally announced one week after each respective codeathon event. &lt;/span&gt;</p> <hr /> </div> <div> <br /> <h4>Online Contest Evaluation </h4> <p><strong>Objective:</strong></p> <ul> <li><strong>Phase 1:</strong> Selection of up to the top 20 submissions from each of the challenge&rsquo;s ten unique contests.</li> </ul> <ul> <li><strong>Phase 2:</strong> Selection of up to the top 5 finalists from each of the challenge&rsquo;s ten unique contests.</li> </ul> <p><strong>Process:</strong>The Online Contest submissions, which include the &ldquo;best and final&rdquo; versions of Regional Codeathon submissions (the online contest only submissions and the &ldquo;best and final&rdquo; versions of Regional Codeathon submissions when combined are referred to as &ldquo;best and final submissions&rdquo;), will be evaluated to select the challenge finalists through the Online Contest evaluation. Phase 1 of the review will select up to the top 20 submissions for each contest. Phase 2 will evaluate these submissions against criteria items 4 and 5. This evaluation will be done between November 16, 2019 and January 24, 2020 through an online judging process. Up to the top 5 submissions for each contest will be invited to participate in the National Award Event and compete for the final prize awards in April 2020. Participants will not be provided oral or written feedback based on the evaluation of their submissions.</p> <h4>Judging Phases</h4> <p><strong><span>Phase 1 - Desk review of the best and final submissions and judging</span></strong></p> <p><span>Criteria \#0-3</span></p> <p><strong><span>Objective:</span></strong><span> Desk review of the best and final submissions based on criteria items 0 to 3 per unique contest. </span></p> <p><strong>Evaluation Process</strong></p> <p>● Judges will be assigned a set of submissions for online evaluation through the program&rsquo;s submission management and evaluation platform.</p> <p>● Judges will evaluate the participant submissions (application form, video file and any attachments and deliverables required) with the specific criteria 0-3 for the contest.</p> <p>● Judges will evaluate the submissions by inputting ratings in a form within the challenge&rsquo;s submission management and evaluation platform.</p> <p>● The judging process may be assisted by reviewers to implement specific tests, assessments, or other procedures.</p> <hr /> <p><strong><span>Phase 2 - Desk review of the submissions, testing, and final judging</span></strong></p> <p><span>Criteria \#4-5</span></p> <p><strong><span>Objective:&nbsp; </span></strong><span>Desk review, testing, and technical assessment of the best and final submissions based on criteria items 4 to 5 per unique contest. </span></p> <p><strong>Evaluation Process</strong></p> <p>● Reviewers will perform an extensive technical assessment and testing of each participant submission selected in phase 1, as defined in evaluation criteria items 4-5 depending on the requirements for each specific contest.</p> <p>● The results of the technical assessment and testing will be used by the judges in making their selections of the National Finalists of each contest.</p> <hr /> </div> <div> <h4>National Awards Event Evaulation </h4> <p>Participants selected in the Online Contest will be invited to the National Awards Event. Participants will prepare for a live pitch session as part of a NIST managed public event, Demonstration (Demo) Day, to showcase their solution, market entry and scale-up strategy, and a 6-month growth plan. During Demo Day, participants will be evaluated by judges appointed by NIST and in accordance with the evaluation criteria specific to this contest.</p> </div> <div> <h4>Evaluation Criteria </h4> <p>The Tech to Protect Challenge includes 10 unique contests &ndash; each with a specific focus. Complete evaluation criteria are included in the <a href="{{ site.baseurl }}/assets/document-library/3.2_TechtoProtectChallenge_Program-Rules.FINAL.4.1.19.pdf" target="_blank" rel="noopener">Official Rules document</a>.</p> </div> </div>
:ET