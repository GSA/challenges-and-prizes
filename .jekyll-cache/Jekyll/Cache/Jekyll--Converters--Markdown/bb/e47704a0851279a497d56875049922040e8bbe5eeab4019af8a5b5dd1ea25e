I",<h4 id="judging-panel">Judging Panel</h4>

<p>See: Judging Criteria.</p>

<h4 id="judging-criteria">Judging Criteria</h4>

<p><strong>Assessment Criteria and Simulation Environment</strong></p>

<p>We seek innovative AI-enabled technologies to optimize Warfighter decision-support at machine speed in relevant, maritime environments. The following sections describe the assessment criteria and the iRIL environment.</p>

<p><strong>Assessment Criteria ‚Äì NetANTX Challenge Invitations</strong></p>

<p>Conforming white paper and quad chart submissions will be evaluated by a panel of qualified experts using the following criteria which is in descending order of importance:</p>

<ul>
  <li>Operational impact of the technology/engineering innovation in the intended mission scenarios and operational environment</li>
  <li>Estimated technical performance (accuracy, precision, recall, F1 score, AUC ROC, etc.) of the technology/engineering innovation against a hold-out dataset</li>
  <li>Integration complexity of the technology/engineering innovation</li>
  <li>Technical maturity of the technology/engineering innovation</li>
</ul>

<p>When conducting the assessment, the Government reserves the right to take other significant factors as required into consideration, such as:</p>

<ul>
  <li>Limitations to use due to Intellectual Property ownership</li>
  <li>Ease of fielding the technology in existing legacy Naval platforms (e.g. solution that requires many large software dependencies or require significant compute/memory resources, or that run on very specific hardware architectures would be viewed less favorably).</li>
  <li>Required out-of-band information needed for the AI technology to operate</li>
</ul>

<p>Based on this initial assessment of the white papers and quad charts, participants will be invited to participate in the challenge. Selected participants will be notified via email communication and will receive invitations to the challenge question and answer (Q&amp;A) Sessions and iRIL Integration Workshops.</p>

<p><strong>Assessment Criteria ‚Äì AI Prize Challenge</strong></p>

<p>Challenge submissions will be evaluated by a panel of qualified AI experts using the following criteria, in descending order of importance:</p>

<ul>
  <li>Operational impact of the technology/engineering innovation in the intended mission scenarios and operational environment</li>
  <li>Technical performance (see below)</li>
  <li>Integration complexity of the technology/engineering innovation</li>
  <li>Technical maturity of the technology/engineering innovation</li>
</ul>

<p>Technical performance will be judged with metrics appropriate for the submission. Metrics may include:</p>

<ul>
  <li>F1 Score</li>
  <li>Precision</li>
  <li>Recall</li>
  <li>Accuracy</li>
  <li>AUC ROC</li>
  <li>Other: teams are encouraged to propose other performance metrics relevant to their submission. Include details of scoring methodology and advantages of the metric for Government review.</li>
</ul>

<p>F1 Score is defined as 2x((precision x recall)/(precision+recall)).</p>

<p>Precision is defined as is the number of true positives (TP) divided by the number of TPs and false positives (FP).</p>

<p>Recall is defined as the number of TPs divided by the number of TPs and the number of false negatives (FNs).</p>

<p>Accuracy is defined as the number of correct predictions made divided by the total number of predictions made, multiplied by 100 to turn it into a percentage.</p>

<p>AUC ROC is defined as ‚ÄúArea under the Receiver Operating Characteristic Curve.‚Äù AUC measures the entire two-dimensional area underneath the entire ROC curve from (0,0) to (1,1). The ROC curve is defined as a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters: True Positive Rate (TPR) = Recall = TP/(TP+FN) and False Positive Rate (FPR) = FP/(FP+TN).</p>

<p>Teams are encouraged to include useful data or performance visualizations.</p>

<p><strong>AI iRIL, Datasets, and Filesets</strong></p>

<p>AI iRIL</p>

<p>Participants will be provided with VDI access to the OSA environment, which is generically referred to as an iRIL. OSA provides the processes, infrastructure, and tools underpinning these subsystems for application providers. The OSA is comprised of the following components:</p>

<ul>
  <li>Naval Research &amp; Development Establishment (NR&amp;DE) Secure Cloud provides Naval R&amp;D labs and its developers with Infrastructure as a Service, Platform as a Service, security services, and development tools via a CCSP.</li>
  <li>Overmatch Software Armory (OSA) Tools and Services is a DON Environment to support accelerated application development while making optimal use of infrastructure.</li>
  <li>Collaborative Staging Environment (CSE) is a DON environment to support accelerated application integration and testing. The CSE provides a cloud-based production-representative test environment for applications to conduct Integration Testing.</li>
  <li>Agile Core Services (ACS) Provide an application infrastructure platform with a diverse set of services to enable easy and rapid delivery of mission applications (provide an environment with common tools to help Applications deploy faster)</li>
  <li>Application Arsenal is an enterprise software distribution, installation, and update service that enables delivery of approved software from Ashore to the Tactical Edge.</li>
</ul>

<p>Technologies that require specific hardware, proprietary components, or on-premises management appliances or consoles will not be considered.</p>

<p><strong>Prize Award Details</strong></p>

<p>Winners will be announced at the conclusion of the AINet ANTX event. NAVWAR will also announce the winners on the Challenge.gov website, the NAVWAR LinkedIN page, media outlets, and social media channels.</p>

<p>NAVWAR has established $100,000 as the total amount set aside for cash prizes under this Challenge. A $75,000 first place cash prize will be awarded to the winning entry. $25,000 second place cash prizes will also be awarded. In the unlikely event of a tie, NAVWAR will determine an equitable method of distributing the cash prizes. In the event that an entity ineligible of receiving the cash award wins first and/or second place, NAVWAR will determine the equitable method of distributing the cash prizes.</p>

<p>If a prize goes to a team of participants, NAVWAR will award the cash prize to the individual/team‚Äôs point of contact registered via the challenge website, for further distribution to the team, as the team members see fit.</p>

<p>NAVWAR is executing two simultaneous but independent prize challenges the AINet ANTX. This prize challenge announcement, and the details within, refer specifically to the AI Prize Challenge. A companion prize challenge, titled the Networks Prize Challenge, has been announced separately on Challenge.gov.</p>
:ET