I"â <p><strong>Scope:&nbsp; </strong></p>
<p>The Challenge evaluation will focus on AI/ML technologies that detect malware on an endpoint. The following describes the scope of the candidate technologies:</p>
<ul> <li>This evaluation is focused on detecting malware at the endpoint. Network-based tools (such as network intrusion detection systems) or network-based systems that reconstruct and analyze files from network traffic are not eligible.</li> <li>Malware detection tools will classify malware on the host:&nbsp; <ul> <li>within the filesystem statically or dynamically, including addition, alteration or replacement of authorized operating system, application, or user files; or</li> <li>with fileless techniques (e.g. identifying fileless malware that sits in memory as executable instructions); or</li> <li>based on other host activities (e.g., logs indicating encrypted command and control communications).</li> </ul> </li> <li>Technologies are expected to have an artificial intelligence and/or machine learning component (e.g., static or dynamic analysis and classifier), with the option to include other complementary approaches, such as signature-based detection.</li> <li>Technologies must operate on-premises, whether solely at the endpoint or in coordination with a local network-level appliance or VM that emulates a cloud analytic capability.</li> <li>To measure how AI/ML improves malware detection, the test data will use public and private benign and malicious samples.</li> </ul>
<p><strong>Test Evaluation Process:&nbsp; </strong></p>
<p>The evaluation team will review the white papers in order to better understand the tool.&nbsp; The actual test evaluation process of the tool will involve the delivery of multiple file test samples to virtual machines (VMs) running the Participant&rsquo;s malware detection technology. For each test sample (benign or malicious file), the following procedures will be performed on the tool:</p>
<ul> <li>A VM will be instantiated with the Participant&rsquo;s malware detection technology installed and running.</li> <li>The following information will be recorded for evaluation during a set time interval in which the sample, a file or fileless malware/benign-ware, is present on the VM:&nbsp; <ul> <li>The classification decision for the sample, under the following constraints:&nbsp; <ul> <li>The label must identify whether benign or malicious and be time stamped. In the event there is no label given in the time interval or the tool does not produce a log or result for the sample, the classification will be interpreted as a benign label with maximum timestamp for that time interval.</li> <li>At most one label shall be given per sample. Other peripheral information beyond the required classification decision (e.g. taxonomy, confidence score, etc.) may also be provided, but will not be used in scoring of the Participant technology.</li> <li>The output classification and any peripheral information must be programmatically accessible by the evaluation team. Documentation of the classification logging approach and format must be provided by the Participant. For example, time-tagged logging content may be:&nbsp; <ul> <li>Sent to a logging server/SIEM, such as Splunk.</li> <li>Logged to syslog, a local file, or windows event logs.</li> </ul> </li> </ul> </li> </ul> </li> <li>Resource usage by only the Participant technology for the duration of the test sample&rsquo;s VM as follows:</li> <li>CPU usage per second</li> <li>Volatile memory usage per second</li> <li>Disk I/O per second</li> <li>Network I/O per second</li> </ul>
<p><strong>Note:</strong> If the Participant&rsquo;s technology uses an on-premises appliance for dynamic or static analysis, or AI/ML analysis, either via a hardware appliance or a virtual machine, resource usage of the management console will be recorded as well as for the VM containing the test sample. If the on-premises appliance only manages endpoint agents, the usage will <strong>not</strong> be recorded.</p>
<p><strong>Scoring:</strong></p>
<p>Each candidate technology will be scored based on the aggregate performance of their technology when tested against benign and malicious file samples. The score will be in terms of a cost estimate, and will sum the following:</p>
<ul> <li><strong>Simulated attack cost:</strong> A cost function will be used to compute estimated attack costs of each malicious file based on the file&rsquo;s negative effects. The functions will be increasing in time, so that early detection will incur less costs than later detection. Note that in the case of a true positive (correct classification of the malicious sample) only the attack cost up to the detection time is accrued. In the case of a false negative (no alert on a malicious sample) the attack cost for the whole test period will be incurred. For false positive (alerting upon a benign sample) and true negative (correctly not alerting upon a benign sample) scenarios, there is no attack and thus no attack costs will be accrued.</li> <li><strong>Simulated security operator cost:</strong> Costs for handling alerts will be computed. This includes only the costs for handling true positive and false positive scenarios, respectively. These will be estimated from actual manpower costs incurred by security operation centers to triage, investigate, and document host alerts. In the case of the true negative or false negative scenario, there are no alerts and hence no operator costs are accrued.</li> <li><strong>Simulated host resource cost:&nbsp; </strong>Costs for the use of host resources will be accrued throughout the duration of the event. The resources monitored on each host are as follows:&nbsp; <ul> <li>CPU usage per second</li> <li>Volatile memory usage per second</li> <li>Disk I/O per second</li> <li>Network I/O per second</li> </ul> </li> </ul>
<p>The simulated costs of each resource itemized above will be estimated from current rates. These simulated resource costs are accrued regardless of correct (true positive, true negative) or incorrect (false positive, false negative) classification. Note that in the case of a true negative (correctly not alerting on a benign sample), only host resource costs (no attack cost and no operator costs) will be accrued.</p>
<p><strong>THE WINNERS OF THE CHALLENGE WILL BE THE PARTICIPANTS WITH THE LOWEST COMPUTED TOTAL COST IN ACCORDANCE WITH THE SCORING ABOVE. </strong></p>
<p><strong>Environment:&nbsp; </strong></p>
<p>The evaluation environment will be VMs. Submitted technologies do not have to be compatible with all tested platforms, but must be compatible with at least one. The two evaluation platforms include the following:</p>
<ul> <li>Windows 10 Enterprise 64-bit</li> <li>CentOS 7 64-bit</li> </ul>
<p>The operating systems will have the following additional user software installed as a baseline:</p>
<ul> <li>Windows:&nbsp; Microsoft Office, Java, Adobe PDF Reader</li> <li>CentOS:&nbsp; OpenOffice, Java, Adobe PDF Reader</li> </ul>
<p>The host-based endpoint agents must be able to be installed on the Windows VM through either a GUI, console, or powershell using standard Windows installation procedures. On CentOS, an RPM-compatible or RHEL-distribution compatible installer must be provided. Documentation describing the installation procedures should also be provided when submitting the tool, in accordance with the tool submission guidelines provided above.</p>
<p>If the technology requires an on-premises management appliance or console, then software and/or hardware components for the management appliance should be provided upon submission of the white paper <strong>no later than 30 September 2019 at 1700 EDT.</strong> The components to run the management console must include documentation necessary for installation and a three-month evaluation license if needed. The software and/or hardware components for the on-premises console must be one of the following:</p>
<ul> <li>Exported Virtual Machine Image (e.g. .ova, .qcow2, etc.) that can be ran with a libvirt-compatible hypervisor (e.g. QEMU, XenServer, VMWare, Virtualbox, Emulab, etc.)</li> <li>Software packages needed to install the appliance, rather than providing the entire virtual machine image.</li> <li>If not an OVA or similar image, a <em>standalone </em>hardware appliance that <em>will not be </em>connected to external cloud services.</li> </ul>
:ET