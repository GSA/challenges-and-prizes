I"üA<h4 id="entry-instructions">Entry Instructions</h4>

<p>All participants must email <a href="mailto:AIATAC.PRIZE.CHALLENGE@NAVY.MIL">AIATAC.PRIZE.CHALLENGE@NAVY.MIL</a> to indicate your intent to participate prior to <strong>22 January 2021 @ 1700 EST</strong>.</p>

<p>Only unclassified information may be part of the submission package. The Phase 1 submission package must include these four items:</p>

<ol>
  <li>Submission description White-paper (Document);</li>
  <li>Overview and Demonstration Videos (2 videos);</li>
  <li>Corresponding SOAR Technology (VM, hardware, and/or any included and necessary software); and,</li>
  <li>Setup Guide (Document)</li>
</ol>

<p>The evaluation team may, but is not required to, contact Participants if a flaw is found in a submission. However, if any submission items are missing or do not meet the following criteria, the Participant will be disqualified.  All submissions must be submitted in their entirety prior to <strong>12 February 2021 @ 1700 EST</strong>.</p>

<p>Only those entries selected for Phase 2 testing will be required to submit the final item:</p>

<ol>
  <li>User Tutorial (1 video)</li>
</ol>

<p>To reiterate, the User Tutorial video is <strong><em>not needed</em></strong> with the initial submission package, and will only be required if advanced to Phase 2. PMW 130 will notify all Contestants that advance to Phase 2;  the Government, may, but is not necessarily required to, notify Contestants that do not advance.</p>

<p>The submission items for both Phase 1 and Phase 2 must meet the following eligibility criteria.</p>

<ol>
  <li>
    <p><strong>White-paper Document Submissions Requirements</strong></p>

    <p>White-papers must use the <a href="{{site.baseurl}}/assets/netlify-uploads/ai-atac3-whitepaper-template.docx">submission template</a>, located at Challenge.gov. It provides the framework for an overview of the proposed technology as well as the following elements:</p>

    <ul>
      <li>Technical approach (e.g. architecture, deployment overview, algorithm description, model description, performance requirements, endpoint footprint, existing results) including descriptions of the AI/ML components; and</li>
      <li>Benefits and novelty of the approach within the context of existing academic and commercially available technologies.</li>
    </ul>

    <p>Sections 1-5 of the white-paper must be collectively no more than six (6) pages in length.</p>

    <p>The white-paper template concludes with a checklist to ensure the submission meets the criteria as detailed in the Challenge Purpose section. Those technologies that do not meet the Scope section criteria may be disqualified or deemed ineligible.  Similarly, for those Participants that do not use the submission template, the Government will not consider the submission.</p>

    <p>Contestants should mark all materials that it considers to be proprietary with a marking of its choosing.  The Government will respect and comply with such markings, if present.  However, any such marking should not prevent the evaluation team from evaluating the Participant submission (Please see the Data Rights section below).  If any videos are submitted which merit a protective marking, the Participant should note which portions of the video(s) are protectable as part of its submission.  <strong><em>Do not submit</em></strong> any Classified information.</p>
  </li>
  <li>
    <p><strong>Demonstration and Overview Videos Requirements</strong></p>

    <p>For Phase 1, each Participant must provide two separate videos â€“ an overview video and a demonstration video, each less than 10 minutes long. The videos can be screen recordings of actual tool usage, an explanatory description with narration, filmed demonstrations, or any other means of showing the solution. Details of required video contents are provided below.</p>

    <p>a. The Demonstration video must <strong>be at most 10 minutes</strong>, and should provide an introduction to the SOAR technologyâ€™s platform, including at a minimum, examples of a user:</p>

    <ul>
      <li>Viewing events in the SOAR UI,</li>
      <li>Using a playbook/workflow to handle an incident,</li>
      <li>How SOC operators can collaborate with each other on an investigation with the tool,</li>
      <li>How tickets are automatically populated with the tool, and</li>
      <li>Orchestration of multiple, related incidents or issues.</li>
    </ul>

    <p>b. The Overview video must <strong>be at most 10 minutes</strong> and should provide an overview of functions and features of the Participantâ€™s technical solution. The Participant may choose any aspects of their tool to showcase, but areas of interest to the U.S. Navy include:</p>

    <ul>
      <li>The Threat Intelligence Platform â€“ how the tool knows of likely threat locations and types; who provides the threat data and how;</li>
      <li>Worthwhile Automated - automatic identification and resolution of simple but voluminous alerts and warnings (accurate methods to reduce repetitive actions by an analyst);</li>
      <li>Incorporated AI/ML - AI/ML tools, components, or processes that can identify unusual threats or conditions, and alert the operators accordingly;</li>
      <li>Flexible Integrations - Interoperability or extensibility with other tools or platforms and/or APIs used;</li>
      <li>Metrics - Examples of metrics generated, especially those that show time saved, value gained, number of incidents handled, et cetera, and</li>
      <li>Offline/Online â€“ How well the tool works offline (i.e. not in the cloud or only on a network without cloud connectivity).</li>
    </ul>

    <p>The videos should demonstrate the SOAR toolâ€™s capability and highlight the value and ease-of-use of the SOAR tool. Any other desired functionality is welcome within the at-most-10-minute videos. Extra videos or any content after 10 minutes in a video will not be reviewed.</p>

    <p><strong>Video format</strong>: Videos must be provided in MP4 file format or provided via a link to be viewed online. If the videos are encrypted or protected, please provide a password.</p>
  </li>
  <li>
    <p><strong>Technology Software and/or VM Submission Requirements</strong></p>

    <p>The software components for SOAR technology must meet the following criteria:</p>

    <ul>
      <li>
        <p><strong>Software Format</strong>: It can be in one of the following formats:</p>

        <ul>
          <li>Software that can be run on modern Windows or Linux OSes (e.g., the U.S. Navy is moving towards all Windows 10 and uses Red Hat Enterprise 7.x in some environments)</li>
          <li>Exported virtual machine (VM) images in .ovf or .ova format that are compatible with VMWare ESXi 6.7 (i.e., they MUST readily import into VMWare, no conversion should be done)</li>
          <li>Docker container packages (i.e., using the Docker tar command)</li>
        </ul>
      </li>
      <li>
        <p><strong>Hardware Format:</strong></p>

        <ul>
          <li>Technologies selected for Phase 2 can provide their technologies pre-installed on hardware server systems or provide co-processing systems for their technology.</li>
        </ul>
      </li>
      <li>
        <p><strong>Cloud and On-Premises (On-prem) Requirements</strong>:  For testing, submitted technologies will be installed on the Cybersecurity Operations Research Range (CORR) evaluation environment at Oak Ridge National Laboratory (ORNL). Due to the potential variety of solution types, the Challenge will accept solutions that require access to external resources as well as solutions that are hosted on-prem. Submitted technologies will be allowed to use the Cloud, via the Internet, to run their software during this challenge.  We expect this to facilitate checking of license keys, checking threat databases, running portions of the code remotely on vendor systems, and so on.  However, NAVWAR and ORNL may be running portions of the challenge in a D-DIL (denied, degraded, intermittent, or limited) environment, to determine the impact of D-DIL on the effectiveness of the tool and the operators running it.</p>

        <p>As such, during a portion of the testing, no connection to the Internet or an external Cloud should be expected, and all technologies and licenses will be evaluated as to their ability to function without connectivity to a Cloud environment or the Internet.</p>

        <p>During a portion of testing, ORNL will test the ability of the submitted technology to facilitate  collaboration of SOCs on different networks using their own instances of the SOAR tool. All networks will be inside CORR (ORNLâ€™s testbed). To test interaction of the SOCs/SOAR tools across networks, there are two options, as follows:</p>

        <ul>
          <li>Internet connections will be provided to enable connections to the Cloud. Cloud provision and configuration must be provided by the submitting team if this option is used. (SOC-A talks to SOC-B via an external connection.)</li>
          <li>Solutions that do not require external Cloud access, but do allow coordinating across connected networks within ORNLâ€™s testbed are permitted, in lieu of Cloud connections. This simulates connecting multiple SOCs all on a separate enclave, e.g., SIPRnet. (SOC-A talks to SOC-B directly on an internal network.)</li>
        </ul>

        <p>The submitter should select the tools and configuration that they believe will best meet the Navyâ€™s need, whether that solution uses a private Cloud, virtual Cloud, an on-prem solution, or some combination of them. The important necessity is that multiple networks with different instances of this SOAR tool must be able to collaborate either using a remote capability (Cloud or external Internet) or via an ORNL-on-prem capability (internal, private networks).</p>

        <p>Regardless of the proposed solution type, all necessary components (software, licenses, configuration / setup instructions, and potentially hardware) must be provided by the submitter.  ORNL will allow external access to resources such as the Cloud, however, it will not provide external Cloud resources, either commercial or Government.</p>
      </li>
      <li><strong>Instances required</strong>: Submissions must provide SOAR software for multiple instances to be configured on CORR, specifically, allowing up to 7 SOC operators per network on up to 3 small networks (each network can be assumed to include fewer than 5,000 IPs, with total bandwidth under 10Gb/s). These instances of the submitted SOAR technology must be able to integrate with each other both within and across networks (e.g., leveraging a virtual cloud or dedicated cross-network connection) to facilitate collaboration of SOC operators within and across networks using this SOAR tool. Any needed corresponding tools (e.g., SIEM of Zeek) must be included in the software / VM / hardware submission package and be as configured as possible. Any remaining configuration needed that may be specific to the test network MUST be provided along with the User Tutorial document if the SOAR tool is advanced to Phase 2.</li>
      <li><strong>Licenses required</strong>: Software licenses for these instances must be valid through December 31, 2021 and must function properly without connectivity to the internet.</li>
      <li>
        <p><strong>Integration Requirements:</strong> Submission must ingest a wide variety of alerts and logs, including but not limited to:</p>

        <ul>
          <li>Host logs including Windows System Logs, Linux syslogs</li>
          <li>Host-based defensive software logs including</li>
          <li>Host firewall logs</li>
          <li>Anti-virus, malware detection, and/or memory access/scripting violation logs</li>
          <li>Endpoint detection and response logs</li>
          <li>Endpoint policy compliance software logs</li>
        </ul>
      </li>
      <li>
        <p>Network-level defensive software logs including</p>

        <ul>
          <li>Network-level firewall logs</li>
          <li>Intrusion detection and prevention system output</li>
          <li>Network-level malware detector alerts</li>
          <li>Network flow sensors</li>
          <li>PCAP (packet capture)</li>
          <li>Zeek logs</li>
        </ul>
      </li>
      <li>Vulnerability scanning tool outputs</li>
      <li>
        <p>Logs produced by network services, including</p>

        <ul>
          <li>Active Directory</li>
          <li>LDAP</li>
          <li>Kerberos</li>
          <li>DNS</li>
          <li>Mail Client</li>
          <li>DHCP</li>
          <li>Threat intelligence platform information, both internally and externally</li>
          <li>Ticketing or other documentation system(s)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Setup Guide Document Requirements</strong></p>

    <p>The setup guide should be a concise, easy-to-follow set of instructions for installing the submissionâ€™s VM/Software/Hardware, configuring integrations with subsidiary SOC tools, and integrating multiple instances of this SOAR tool. A technical Point of Contact (POC) for assisting with proper setup and configuration should be included in the white-paper, along with the POCâ€™s phone number and email.</p>

    <ul>
      <li><strong>Setup time requirements</strong>: All required components should be submitted as configured and integrated as much as possible. Submissions must provide sufficiently mature software, documentation (setup guide), and support to the ORNL test team to ensure that the submitted SOAR instances <strong>can be configured for use on the up-to-three small networks with at most 16 labor hours of support.</strong> Submissions requiring more than 16 labor hours for setup and configuration are subject to disqualification. It is critical that the technical POC be available during business hours during the challenge to avoid disqualification.</li>
      <li><strong>Setup Support:</strong> The Submission team should be prepared to provide configuration support to the ORNL test team remotely (e.g. via phone or video conference) to facilitate proper setup and configuration. Setup configuration and support (virtual) meetings will be scheduled in advance of a needed meeting to accommodate the Challenge schedule.</li>
    </ul>
  </li>
  <li>
    <p><strong>User Tutorial Video</strong></p>

    <p>The User Tutorial Video is <strong>not</strong> required with Phase 1 submission. This will only be required of those Participant technologies selected for Phase 2. Further instructions and deadlines will be communicated to these Participants.</p>

    <p>Submissions selected for Phase 2 testing must provide a training or tutorial video that teaches a new user (SOC operator) how to use the (already set up and configured) SOAR tool. The tutorial should take the user <strong>under 2 hours</strong> to complete and should assume that the tutorial user has a bachelor degree education level and 0-5 years of experience in cybersecurity or related field, and no experience in the specific SOAR tool. The tutorial video may further assume the user is using an instance of the submitted SOAR tool. The User Tutorial will be provided directly to both ORNL and U.S. Navy operators. In particular, the following is to be included:</p>

    <ul>
      <li>Instructions for basic use of the user interface, including how to query or otherwise visualize ingested logging and alerting data and threat intelligence information.</li>
      <li>Instructions for how to follow a pre-set workflow or playbook for triage and/or incident response.</li>
      <li>Instructions for how to create a new workflow or playbook.</li>
      <li>Instructions for how to use document current status of an investigation (e.g. using an integrated ticketing system).</li>
      <li>Instructions for working collaboratively with other SOC operators.</li>
      <li>Instructions for handing off a current investigation to other SOC operators.</li>
    </ul>
  </li>
</ol>

<p>More details will be provided to the Participants selected for Phase 2.</p>

<p><strong>Video format:</strong> Videos must be provided in MP4 file format or provided via a link to be viewed online. If the videos are encrypted, please provide a password.</p>

<p>Submission packages meeting the criteria above must be shipped by trackable, non-postal delivery (FedEx, UPS, DHL, etc.) and received <strong>no later than 12 February 2021 at 1700 EST</strong>, to the following address:</p>

<p><strong>For courier services (e.g., FedEx, UPS) use:</strong>
Cybersecurity Research Group
Oak Ridge National Laboratory
Attn: AI ATAC Evaluation Team
1 Bethel Valley Road Bldg 6012, Room 209
Oak Ridge, TN 37830</p>

<p>Late submissions will be disqualified and will not evaluated.</p>
:ET