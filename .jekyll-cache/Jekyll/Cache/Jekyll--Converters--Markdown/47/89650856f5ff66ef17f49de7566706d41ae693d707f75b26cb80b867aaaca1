I"D<h1 id="judging-criteria">Judging Criteria</h1>

<p><strong>Judging Panel</strong></p>

<p>Technical reviewers with expertise relevant to the Challenge will evaluate the solutions based on their ability to achieve the criteria listed below. Technical reviewers will be comprised of experts sourced from the Federal government, academia, industry, and other sectors as deemed relevant. The solutions and evaluation information from the technical reviewers will then be reviewed by federal employees serving as judges, who will select up to 3 challenge winners, as well as any honorable mentions subject to the final decision by the Award Approving Official.</p>

<p>The award approving official will be the BARDA director.</p>

<p><strong>Evaluation Metrics</strong></p>

<p>Each submission will be evaluated along quantitative and qualitative metrics.</p>

<p><strong>Quantitative Metrics</strong></p>

<p>Participants in this challenge will build models on a training dataset established by challenge organizers. Those trained models will then be tested on a holdout set to establish initial model performance. Models will then be put through a “final training”. The primary metrics used to score models will be the Fβ metric, followed by the Area Under the Precision Recall Curve and the Area Under the Receiver Operator Curve. The AUROC will be used to evaluate model generalizability, where models will be evaluated for temporal, cross-site, and demographic generalizability.</p>

<p><strong>Qualitative Metrics</strong></p>

<p>Qualitative metrics of utility and reproducibility will also be used as part of the final ranking of the submissions. Factors related to model utility include (1) feature interpretation (Can the factors driving individual predictions be understood or is the model a black box?), (2) method clarity (Are the code, documentation, and final submission write up understandable and clear?), (3) timeliness of predictions (are models able to make accurate predictions before an intervention needs to happen), and (4) context utility (can a model still make accurate predictions in the setting in which it will be used?). Factors related to reproducibility include (1) technical reproducibility (Can the code be deployed and re-run successfully under different computational conditions?), (2) prediction reproducibility (Does the model make the same predictions each time under the same conditions?), and (3) documentation reproducibility (Is there sufficient documentation to replicate the model?).</p>

<p><strong>Judging Criteria</strong></p>

<p>The above metrics will become part of a ranking consensus for the submissions in each of the challenge questions. The primary purpose of this challenge is to identify the most clinically useful predictions models for pediatric severity of COVID-19. The judges will use both the quantitative metrics and the qualitative metrics to identify the models submitted to each question that are most likely to be clinically useful.</p>

<p>For more information: <a href="https://www.synapse.org/peds_covid_severity">https://www.synapse.org/peds_covid_severity</a></p>
:ET