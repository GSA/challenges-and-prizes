I"þ<p>Buildings around the world are as diverse as the conditions they face. The xBD dataset includes pre- and post-disaster imagery for six different types of disaster and fifteen countries.</p>
<p><strong>Goal:</strong></p>
<p>The Challenge requires solvers to provide a computational solution that accurately localizes buildings in overhead imagery and scores the severity of building damage.</p>
<p><strong>Ranking Solvers:</strong></p>
<p>(A) Highest Score First. Solvers will be ranked in descending order, with highest score first and lowest score last. Further detail on the scoring methodology is available on the&nbsp;<a href="https://xview2.org/challenge">Challenge</a>&nbsp;and&nbsp;<a href="https://xview2.org/rules">Rules</a>&nbsp;pages on this site.</p>
<p>(B) Ties Ranked by Submission Time. In the event of a tie between participants, the one with the earliest submission time will be ranked first while the one with the later submission time will be ranked lower. DIU reserves the right to break ties when necessary based on finer time precision than displayed on the public leaderboard.</p>
<p><strong>Inputs</strong></p>
<p>Given a pair of pre/post images, your model must localize and classify building damage. Input images are square RGB image files in PNG format, with height and width of 1024 pixels. Pre/post pairs are identified by matching numerical IDs for each set of pre/post filenames. For the training data, filenames also include information about the disaster, but disaster information is obfuscated in the test dataset.</p>
<p><strong>Outputs</strong></p>
<p>Your model must predict an output PNG image with height and width of 1024 pixels, where each pixel value corresponds to the predicted class at that place in the input image:</p>
<ul> <li>0:&nbsp; no building</li> <li>1:&nbsp; undamaged building</li> <li>2:&nbsp; building with minor damage</li> <li>3:&nbsp; building with major damage</li> <li>4:&nbsp; destroyed building</li> </ul>
<p>Localization is scored against the building polygons annotated in the "pre" images, at each pixel location. Damage classification is scored against the damage levels annotated in the "post" images, at each pixel location.</p>
<p><strong>Baseline model</strong></p>
<p>A public baseline model is currently being evaluated to establish public benchmark performance levels. The code and results for the baseline model will be available soon.</p>
<p><strong>Upload Your Predictions</strong></p>
<p>The first step in a submission is uploading your predictions for the test dataset, which you should compute offline on your own. Download the test dataset, compute a prediction for each of the instances in the test set, and then upload your predictions for evaluation and display on the public leaderboard. You may elect to upload submissions anonymously, which will display "Anonymous" for that submission on the leaderboard. Solvers may upload one submission at a time; a maximum of three submissions per day will be evaluated. Baseline models are currently being evaluated to establish public benchmark performance levels; leaderboard functionality will be available soon.</p>
<p><strong>Submit your Container for Verification</strong></p>
<p>The best results on the leaderboard will be eligible for online container verification; a successful container verification is required to complete a submission and be eligible for awards. To submit your container for verification, you must containerize your code to compute one prediction, push your container to Docker Hub in a public or private account, authorize the Challenge Sponsor to pull your container, and then use the form to submit a container verification job. If the results of your container evaluation are the same as your previously uploaded predictions, your container is successfully verified. A tutorial and further details will be available when leaderboard functionality is active. Container verification submissions may be made from public or private repositories. Container verification is part of the submission process, and a successful container verification is required to be eligible for awards.</p>
<p><strong>Evaluation Metric</strong></p>
<p>The overall ranking metric for the xView2 Challenge is combined F1 score. Submissions are evaluated over the test dataset to compute a localization F1 score and a damage classification F1 score. Localization F1 scores the agreement between your predictions (0 = no building, or 1-4 = building) at each pixel location versus the ground truth labels for the "pre" image. Damage classification F1 scores the agreement between your predictions (1 = no damage, 2 = minor damage, 3 = major damage, 4 = destroyed) over the pixels of each building polygon in the ground truth of the "post" images. The overall F1 score is 30% localization F1 + 70% damage classification F1.</p>
:ET