I"P<p>The Challenge evaluation will focus on AI/ML technologies that detect adversarial campaigns via network observable behaviors or by analysis of data collected across an enterprise.</p>
<p>The following describes the network being defended in this Challenge:</p>
<ul> <li>An enterprise network with a relatively flat architecture of maximum 1,500 endpoints.</li> <li>All north/south and east/west traffic is observable with a mixture of encrypted and unencrypted traffic. Traffic decryption is not provided.</li> <li>Network services include (minimally) a Firewall, Active Directory, Domain Name Service, and Dynamic Addressing (DHCP), and the network data will contain traffic from these services.</li> <li>Enterprise endpoint defense is a commercial, enterprise-grade endpoint security service product, offering host protection. The candidate product will not receive information directly from or rely upon the host endpoint security service.</li> <li>The environment will have a SIEM, potentially with relevant service logs and endpoint logs that will be available for query.</li> <li>Specific data and formats will be shared at the time of technology installation and configuration (see Challenge Timeline &amp; Evaluation Process).</li> </ul>
<p>The following describes the scope of the candidate technologies:</p>
<ul> <li>This evaluation is focused on detecting adversarial campaigns against a small/medium enterprise network using network observable data and events, with normal proportions of encrypted and unencrypted content.</li> <li>Endpoint detection and protection tools (such as anti-virus or host-based malware detectors), source code analyzers, reverse engineering frameworks, or operations automation frameworks are not eligible.</li> <li>Technologies will be provided a passive tap for the enterprise network, and an ability to interact with the SIEM.</li> <li>Technologies will analyze raw network data streams to determine the presence of an adversarial campaign or individual attack events.</li> <li>Technologies will generate network alerts, ingestible by the SIEM and using an alert format compatible with common SIEMs (e.g. Splunk).</li> <li>Technologies are expected to have an artificial intelligence and/or machine learning component and can also include other complementary approaches, such as signature- or rule-based detection.</li> <li>To measure how AI/ML improves adversarial campaign detection, the test data will use commercially available and custom developed campaigns using various exploitation frameworks.</li> <li>For each adversarial campaign, the range and associated analytic technologies will be reset to a known state.</li> <li>Technologies that leverage learning online or from historical data will be provided three weeks of ambient network data to form their models.</li> <li>Technologies that rely on learning online or from historical data must provide the capability to snapshot, or save, and then reload a built model.</li> <li>Technologies must operate completely on-premises. There will be no external connectivity available during the Challenge. Technologies that require an external/cloud connection will be disqualified.</li> <li>Technologies are expected to provide visibility into their resource usage and computational performance in an easily accessible manner.</li> </ul>
<p><strong>Environment</strong></p>
<p>The evaluation environment will be the Cybersecurity Operations Research Range (CORR) at Oak Ridge National Laboratory (ORNL). Submitted technologies must include documentation necessary for installation and configuration, and also an evaluation license that covers minimally the following conditions:</p>
<ul> <li>A trial period through December 2020,</li> <li>A maximum 10 Gb/s supported network data rate, and/or</li> <li>Supporting traffic from a network up to 1,500 nodes in size.</li> </ul>
<p>The software and/or hardware components for the on-premises console must be one of the following:</p>
<ul> <li>Exported Virtual Machine Image (e.g., .ova, .qcow2, etc.) that can be run with a libvirt-compatible hypervisor (e.g. QEMU, XenServer, VMWare, Virtualbox, Emulab, etc.)</li> <li>Docker container package</li> <li><em>Standalone&nbsp;</em>hardware appliance that&nbsp;<strong><em><u>will not</u></em></strong><em> be&nbsp;</em>connected to external cloud services</li> </ul>
<p><strong>Challenge Evaluation Process</strong></p>
<p>The evaluation process will include four major steps once submissions are received.</p>
<ol> <li><strong>White Paper Analysis.</strong> ORNL&rsquo;s team with consultation of NAVWAR will review the submission of white papers to determine eligibility and down select to a subset of the technologies to be evaluated.</li> <li><strong>Technology Installation and Configuration.</strong> For those submissions remaining, the evaluation team will attempt to install and configure each technology in the range environment, in order to assure communication with the range data interfaces and tune each technology&rsquo;s configuration. A maximum two-day on-site support by the submitting organization may be required to optimize the configuration. At the conclusion of the Installation and Configuration stage, the technology configuration will be locked.</li> <li><strong>Training &amp; Tuning:</strong> The generation of ambient data for ML model training will happen once the range configuration is locked. Each technology will be exposed to approximately three weeks of traffic for ML model generation.</li> <li><strong>Evaluation:</strong> The actual test evaluation process of the tool will involve levying multiple adversarial campaigns against the enterprise, with increasing complexity and realism. Technologies will be evaluated on their ability to detect elements of each campaign as it progresses. Resource information (CPU usage, memory usage, disk I/O, network I/O) associated with the operation of the technology will be collected during each test within the evaluation.</li> </ol>
<p><strong>Scoring</strong></p>
<p>This challenge seeks to test how much of an adversarial campaign (sequence of events towards an exploitative goal) the candidate technologies can uncover. The score will be in terms of a cost estimate that simulates the cost to an enterprise using this technology for a given period of time. The cost estimate will sum simulated attack costs, labor costs, and resource costs.</p>
<ul> <li><strong>Simulated attack cost:</strong>&nbsp;Estimated costs resulting from each campaign&rsquo;s malicious events will be a function representing attack costs over time. This simulates costs of lost or corrupted data, ransoms, etc. The attack cost function is increasing in time and in steps of the attack kill chain, such that early detection will accrue less cost than later (or no) detection. Simulated attack cost is only accrued for a true positive (alerting on a malicious event) until time of detection, and for a false negative (no alert on a malicious event) resulting in a maximum attack cost.</li> <li><strong>Simulated security operator cost:</strong>&nbsp;Estimated SOC costs, based on actual labor rates of security operators, will be computed for initial setup and ongoing use of the technology. For each alert issued by the technology, labor costs for triage, investigation, and incident response will be incurred. Ongoing security operator costs are incurred for both true positives (alerting on malicious events), and false positives (alerting on benign events).</li> <li><strong>Simulated</strong> <strong>resource cost:&nbsp;</strong>Estimated resource costs for initial setup and ongoing use of the technology will be based on rates from recent research. Ongoing resources that will be monitored and incorporated may include but are not limited to usage of CPU, volatile memory, disk I/O, network I/O.</li> </ul>
<p><strong>THE WINNERS OF THE CHALLENGE WILL BE THE PARTICIPANTS WITH THE LOWEST COMPUTED TOTAL COST IN ACCORDANCE WITH THE SCORING ABOVE.</strong></p>
:ET